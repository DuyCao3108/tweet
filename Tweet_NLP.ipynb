{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub  as hub\n",
    "import sys\n",
    "sys.path.append('models')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from official.nlp.data import classifier_data_lib\n",
    "from official.nlp.bert import tokenization\n",
    "from official.nlp import optimization\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Admin\\tweet\\train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, eval_df = train_test_split(df, test_size = 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_df.text.values,train_df.target.values))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((eval_df.text.values,eval_df.target.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [0,1]\n",
    "max_seq_length = 150\n",
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2', trainable = True) \n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file = vocab_file, do_lower_case = do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This provides a function to convert row to input features and label\n",
    "def to_feature(text, label, label_list = label_list, max_seq_length = max_seq_length, tokenizer= tokenizer):\n",
    "    example = classifier_data_lib.InputExample(\n",
    "        guid = None,\n",
    "        text_a = text.numpy(),\n",
    "        text_b =None,\n",
    "        label = label.numpy()\n",
    "    )\n",
    "    feature = classifier_data_lib.convert_single_example(\n",
    "        0,\n",
    "        example,\n",
    "        label_list,\n",
    "        max_seq_length,\n",
    "        tokenizer\n",
    "    )\n",
    "    return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature_map(text, label):\n",
    "\n",
    "    input_ids, input_mask, segment_ids, label_id = tf.py_function(\n",
    "        to_feature,\n",
    "        inp = [text,label],\n",
    "        Tout = [tf.int32,tf.int32, tf.int32, tf.int32 ]\n",
    "    )\n",
    "\n",
    "    input_ids.set_shape([max_seq_length])\n",
    "    input_mask.set_shape([max_seq_length])\n",
    "    segment_ids.set_shape([max_seq_length])\n",
    "    label_id.set_shape([])\n",
    "\n",
    "    x = {\n",
    "        'input_word_ids': input_ids,\n",
    "        \"input_mask\": input_mask, \n",
    "        \"input_type_ids\": segment_ids\n",
    "    }\n",
    "\n",
    "    return (x, label_id)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_data = (\n",
    "    train_data.map(to_feature_map,\n",
    "                   num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "    .shuffle(1000)\n",
    "    .batch(32,drop_remainder = True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  )\n",
    "\n",
    "# valid\n",
    "valid_data = (valid_data.map(to_feature_map,\n",
    "                               num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    ".batch(32, drop_remainder = True)\n",
    ".prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    input_word_ids = tf.keras.layers.Input(shape =(max_seq_length,),dtype = tf.int32,\n",
    "                                           name='input_word_ids')\n",
    "    input_mask = tf.keras.layers.Input(shape =(max_seq_length,),dtype = tf.int32,\n",
    "                                           name='input_mask')\n",
    "    input_type_ids = tf.keras.layers.Input(shape =(max_seq_length,),dtype = tf.int32,\n",
    "                                           name='input_type_ids')\n",
    "    \n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
    "    drop = tf.keras.layers.Dropout(0.4)(pooled_output)\n",
    "    output = tf.keras.layers.Dense(1,activation = 'sigmoid', name = \"output\")(drop)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs ={\n",
    "            'input_word_ids': input_word_ids,\n",
    "            'input_mask': input_mask,\n",
    "            'input_type_ids': input_type_ids\n",
    "        },\n",
    "        outputs = output\n",
    "    )\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics = [tf.keras.metrics.BinaryAccuracy()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model_BERT NLP\n",
    "epochs = 4\n",
    "his = model.fit(\n",
    "    train_data,\n",
    "    validation_data = valid_data,\n",
    "    epochs = epochs,\n",
    "    verbose = 1\n",
    ") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
